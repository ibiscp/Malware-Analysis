{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\\begin{center}\n",
    "by Ibis Prevedello (1794539)\n",
    "\\end{center}\n",
    "\n",
    "# Introduction\n",
    "The goal of this project is to train different classifiers in order to separate malware from non malware aplications running in an Android OS, given features extracted from the `manifest.xml` file and from the disassembled code.\n",
    "\n",
    "The dataset that is being used here is the [The Drebin Dataset](https://www.sec.cs.tu-bs.de/~danarp/drebin/download.html) from the Braunschweig University of Technology.\n",
    "\n",
    "The dataset contains 5,560 files from 179 different malware families. The samples were collected in the period of August 2010 to October 2012. More details on the dataset can be found in the [paper](https://www.tu-braunschweig.de/Medien-DB/sec/pubs/2014-ndss.pdf) describing Drebin and the corresponding evaluation.\n",
    "\n",
    "# Development\n",
    "Below the development of the project is detailed, commenting each of the functions and steps given to achieve the desired goals for the project.\n",
    "\n",
    "## Import necessary libraries for the project\n",
    "In the following block is listed all the necessary libraries for the project, the seed for the random numbers and the path for the folder containing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Difine libraries\n",
    "from collections import defaultdict, OrderedDict\n",
    "from urllib.parse import urlsplit\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "import csv\n",
    "import os, os.path\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seed for random numbers\n",
    "random.seed(1)\n",
    "\n",
    "# Define folder where the log files are located\n",
    "folder = 'drebin/feature_vectors/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Understanding the dataset\n",
    "As described above, the dataset is composed of examples of malware and non-malware data, each file contains features of requested hardware components, requested permissions, app components, network addresses and so on. An example of one file is presented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File:  d4e03d6e85b2f45a754240136d7544351b733fb12d9bb7511227da31bc709399 \n",
      "\n",
      "activity::.PictureMapper\n",
      "feature::android.hardware.touchscreen\n",
      "intent::android.intent.action.MAIN\n",
      "api_call::android/content/ContentResolver;->query\n",
      "activity::Screen2\n",
      "permission::android.permission.INTERNET\n",
      "api_call::android/app/Activity;->startActivity\n",
      "intent::android.intent.category.LAUNCHER\n",
      "real_permission::android.permission.READ_CONTACTS\n"
     ]
    }
   ],
   "source": [
    "sample_file = \\\n",
    "    'd4e03d6e85b2f45a754240136d7544351b733fb12d9bb7511227da31bc709399'\n",
    "    \n",
    "print('File: ', sample_file, '\\n')\n",
    "\n",
    "# Print its content\n",
    "with open(folder + sample_file) as f:\n",
    "    content = f.read().splitlines()\n",
    "    for line in content:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Just to start to have a better understanding about the dataset, it is interesting to find out the total number of malware and non-malware files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset:  129013\n",
      "Number of non malwares:\t 123453\n",
      "Number of malwares:\t 5560\n"
     ]
    }
   ],
   "source": [
    "# Define list for the malware and non malware files\n",
    "non_malware = list()\n",
    "malware = list()\n",
    "\n",
    "# List of all the files\n",
    "dataset = os.listdir(folder)\n",
    "\n",
    "# Create malware dictionary with the file name and the type of each one\n",
    "with open('drebin/sha256_family.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)\n",
    "    malware_dictionary = {row[0]:row[1] for row in reader}\n",
    "\n",
    "# Separate the file names between malware and non malware\n",
    "for i in dataset:\n",
    "    if i in malware_dictionary:\n",
    "        malware.append(i)\n",
    "    else:\n",
    "        non_malware.append(i)\n",
    "    \n",
    "print('Size of dataset: ', len(malware) + len(non_malware))\n",
    "print('Number of non malwares:\\t', len(non_malware))\n",
    "print('Number of malwares:\\t', len(malware))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It is possible to notice that the dataset is not balanced, in this dataset, the number of non malware files is much larger than the number of malware, and this can influence on the classification. So, before to start it is good to pick randomly the same number of files from the non malware class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Balancing the dataset\n",
    "Randomly select the same number of malware examples from the non malware in order to have a balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of non malware:  5560\n",
      "Number of malware:  5560\n"
     ]
    }
   ],
   "source": [
    "# Generate random numbers\n",
    "index = random.sample(range(0, len(non_malware)-1), len(malware))\n",
    "\n",
    "# New list with malware and non malware examples divided \n",
    "non_malware = [non_malware[i] for i in index]\n",
    "\n",
    "# Merged list containing malware and non malware\n",
    "data = malware + non_malware\n",
    "\n",
    "# Vector with class of each example\n",
    "y = [1]*len(malware) + [0]*len(non_malware)\n",
    "\n",
    "# Print number of examples in each class\n",
    "print('Number of non malware: ', len(non_malware))\n",
    "print('Number of malware: ', len(malware))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now both classes have the same number of examples, both with 5560 files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Malware categories\n",
    "Just to have an idea of the dataset, below is presented a list of how many samples there are in each malware class. This data will not be used for this project, but further development can be done in order to classify the class each malware belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of entries in each class of malware (values above 20):\n",
      "\t 925 FakeInstaller\n",
      "\t 667 DroidKungFu\n",
      "\t 625 Plankton\n",
      "\t 613 Opfake\n",
      "\t 339 GinMaster\n",
      "\t 330 BaseBridge\n",
      "\t 152 Iconosys\n",
      "\t 147 Kmin\n",
      "\t 132 FakeDoc\n",
      "\t  92 Geinimi\n",
      "\t  91 Adrd\n",
      "\t  81 DroidDream\n",
      "\t  70 ExploitLinuxLotoor\n",
      "\t  69 Glodream\n",
      "\t  69 MobileTx\n",
      "\t  61 FakeRun\n",
      "\t  59 SendPay\n",
      "\t  58 Gappusin\n",
      "\t  43 Imlog\n",
      "\t  41 SMSreg\n",
      "\t  37 Yzhc\n",
      "\t  29 Jifake\n",
      "\t  28 Hamob\n",
      "\t  27 Boxer\n",
      "\t 775 Others\n"
     ]
    }
   ],
   "source": [
    "print('\\nNumber of entries in each class of malware (values above 20):')\n",
    "\n",
    "v = defaultdict(list)\n",
    "for key, value in sorted(malware_dictionary.items()):\n",
    "    v[value].append(key)\n",
    "ordered_v = \\\n",
    "    OrderedDict(sorted(v.items(), key=lambda x: len(x[1]), reverse=True))\n",
    "count_malware = 0\n",
    "for k in ordered_v:\n",
    "    if len(v[k])>20: # Print only classes with more than 20 examples\n",
    "        count_malware += len(v[k])\n",
    "        print('\\t', '{:>3}'.format(len(v[k])), k)\n",
    "        \n",
    "print('\\t', '{:>3}'.format(len(malware) - count_malware), 'Others')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Features categories\n",
    "\n",
    "As explained above, each file contains all the features extracted from the application. These features are extracted from the `manifest.xml` file and from the disassembled code.\n",
    "\n",
    "Below is presented the total number of categories found in the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The features can be divided in 10 different sets:\n",
      "\t api_call\n",
      "\t feature\n",
      "\t url\n",
      "\t service_receiver\n",
      "\t permission\n",
      "\t call\n",
      "\t intent\n",
      "\t real_permission\n",
      "\t activity\n",
      "\t provider\n"
     ]
    }
   ],
   "source": [
    "# Collect all the features found in the first 20 files of the dataset\n",
    "category_list = list()\n",
    "for file in dataset[0:20]:\n",
    "    with open(folder + file) as f:\n",
    "        content = f.readlines()\n",
    "        for line in content:\n",
    "            category, string = line.split('::')\n",
    "            if category not in category_list:\n",
    "                category_list.append(category)\n",
    "                \n",
    "print('The features can be divided in', \\\n",
    "      len(category_list), 'different sets:')\n",
    "print('\\t','\\n\\t '.join(category_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In order to make the problem easier or more difficult to solve, it is possible to chose which features will be considered in order to classify between the two classes.\n",
    "\n",
    "In the list of features below it is possible to set `True` or `False` for each one of them, `True` to be considered by the classifier and `False` to be discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "features = {\n",
    "    'api_call': True,\n",
    "    'feature': True,\n",
    "    'url': True,\n",
    "    'service_receiver': True,\n",
    "    'permission': True,\n",
    "    'call': True,\n",
    "    'intent': True,\n",
    "    'real_permission': True,\n",
    "    'activity': True,\n",
    "    'provider': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Functions to extract features\n",
    "The functions listed below are used to extract the data necessary from the features. It receives each line of the files based on the category and returns a sequence of words that can be used to create the dictionary and/or construct the vector of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Extract url\n",
    "def extract_url(string):\n",
    "    try:\n",
    "        base_url = \"{0.scheme}://{0.netloc}/\".format(urlsplit(string))\n",
    "        if len(base_url) > 10:\n",
    "            return [base_url]\n",
    "    except:\n",
    "        #print('Error html: ', string)\n",
    "        return None\n",
    "    \n",
    "# Extract api_call\n",
    "def extract_api_call(string):\n",
    "    try:\n",
    "        string = string.replace(';->', '/')\n",
    "        api_call = string.split('/')\n",
    "        return api_call\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "# Extract feature\n",
    "def extract_feature(string):\n",
    "    try:\n",
    "        feature = string.split('.')[-1]\n",
    "        return [feature]\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "# Extract permission and real_permission\n",
    "def extract_permission(string):\n",
    "    try:\n",
    "        permission = string.split('.')[-1].lower()\n",
    "        return [permission]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Extract call\n",
    "def extract_call(string):\n",
    "    try:\n",
    "        call = string.lower()\n",
    "        return [call]\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "# Extract activity\n",
    "def extract_activity(string):\n",
    "    try:\n",
    "        activity = string.split('.')[-1].lower()\n",
    "        return [activity]\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "# Extract intent\n",
    "def extract_intent(string):\n",
    "    try:\n",
    "        intent = string.split('.')[-1].lower()\n",
    "        return [intent]\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "# Extract service_receiver\n",
    "def extract_service_receiver(string):\n",
    "    try:\n",
    "        service_receiver = string.split('.')[-1].lower()\n",
    "        return [service_receiver]\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "# Extract provider\n",
    "def extract_provider(string):\n",
    "    try:\n",
    "        provider = string.split('.')[-1].lower()\n",
    "        return [provider]\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Map words to index\n",
    "It is important to create a dictionary with all the words containing in the files and convert each file to a vector of the index of each word in the dictionary, as show in the example below:\n",
    "```\n",
    "000068216bdb459df847bfdd67dd11069c3c50166db1ea8772cdc9250d948bcf\n",
    "[3, 55, 56, 11, 13, 57, 22, 58, 59, 52, 60, 61, 4, 5, 6, 62, 63, 64, 49, 50, 51]\n",
    "```\n",
    "The following function receives a file, reads its line, process using the functions to extract features and creates or the dictionary or the vector shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create dictionary or list of words\n",
    "def process_file(file, dictionary_creation = False):\n",
    "\n",
    "    # List of words of each file\n",
    "    words = list()\n",
    "    \n",
    "    # Read line by line of the file\n",
    "    with open(folder + file) as f:\n",
    "        content = f.readlines()\n",
    "        \n",
    "        # Divide each line of document in \n",
    "        for line in content:\n",
    "            try:\n",
    "                split = line.split('::')\n",
    "                category = split[0]\n",
    "                string = split[1]\n",
    "            except:\n",
    "                break\n",
    "                \n",
    "            # Only process the categories selected by the user\n",
    "            if (features[category]):\n",
    "                if (category == 'url'):\n",
    "                    word_list = extract_url(string)\n",
    "                elif (category == 'api_call'):\n",
    "                    word_list = extract_api_call(string)\n",
    "                elif (category == 'feature'):\n",
    "                    word_list = extract_feature(string)\n",
    "                elif (category == 'permission' or \\\n",
    "                      category == 'real_permission'):\n",
    "                    word_list = extract_permission(string)\n",
    "                elif (category == 'call'):\n",
    "                    word_list = extract_call(string)\n",
    "                elif (category == 'activity'):\n",
    "                    word_list = extract_activity(string)\n",
    "                elif (category == 'intent'):\n",
    "                    word_list = extract_intent(string)\n",
    "                elif (category == 'service_receiver'):\n",
    "                    word_list = extract_service_receiver(string)\n",
    "                elif (category == 'provider'):\n",
    "                    word_list = extract_provider(string)\n",
    "\n",
    "                # If able to extract feature from line\n",
    "                if word_list != None:\n",
    "                    for word in word_list:\n",
    "                        word = word.replace('\\n', '')\n",
    "\n",
    "                        # If flagged to create the dictionary\n",
    "                        if dictionary_creation:\n",
    "                            index = len(dictionary)-1\n",
    "                            dictionary[word] = index\n",
    "                        else:\n",
    "                            index = dictionary[word]\n",
    "                            if index not in words:\n",
    "                                words.append(index)\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Dictionary creation\n",
    "In order to classify the files, first a dictionary needs to be created using the words of the files that it will classify. For the dictionary all the words containing in each file is being used for each of the categories showed above.\n",
    "As this creating takes time, because it is necessary to read all the words containing in each file, the dictionary is being save to a file called `dictionary.pkl`. It will only be create if it is not found on the directory folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary file found!\n",
      "Dictionary size:  28959\n"
     ]
    }
   ],
   "source": [
    "# Save dictionary to file\n",
    "def save_dic(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Load dictionary from file\n",
    "def load_dic(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# Check if dictionary exists\n",
    "if os.path.isfile('dictionary.pkl'):\n",
    "    print('Dictionary file found!')\n",
    "    dictionary = load_dic('dictionary')\n",
    "\n",
    "else:\n",
    "    print('Dictionary file not found!')\n",
    "    \n",
    "    # Define the dictionary\n",
    "    dictionary = {}\n",
    "    \n",
    "    # Colect words for malware\n",
    "    pbar = tqdm(range(len(data)))\n",
    "    pbar.set_description('Creating dictionary')\n",
    "    for i in pbar:\n",
    "        process_file(data[i], True)\n",
    "        \n",
    "    # Save dictionary\n",
    "    save_dic(dictionary, 'dictionary')\n",
    "    print('\\nDictionary saved to file!')\n",
    "\n",
    "# Print number of words in the dictionary\n",
    "dictionary_size = len(dictionary)\n",
    "print('Dictionary size: ', dictionary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Features extraction\n",
    "This is the function used to extract features of each file, it uses the same function to build the dictionary, but here a vector of zeros is created with the length of the dictionary, and its value is set to one only in the position of the words that contains in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|███████████████████████████████████████████████████████| 11120/11120 [07:17<00:00, 25.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# Construct the vector of features\n",
    "def features_extraction(file):\n",
    "    indices = process_file(file)\n",
    "    \n",
    "    feat = [0] * dictionary_size\n",
    "    \n",
    "    for i in indices:\n",
    "        feat[i-1] = 1\n",
    "\n",
    "    return feat\n",
    "\n",
    "# List of features\n",
    "X = list()\n",
    "\n",
    "# Extract features and append to the list of features\n",
    "pbar = tqdm(range(len(data)))\n",
    "pbar.set_description('Extracting features')\n",
    "for i in pbar:\n",
    "    feat = features_extraction(data[i])\n",
    "    X.append(feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train test split\n",
    "In order to have a good evaluation of the used algorithm, it is important to separate the dataset between training and test set. The training set will be used to train the classifier and the test set will only be used to calculate the metrics.\n",
    "\n",
    "It is important for the classifier do not see the test set before, because it could overfit the data and not be able to generalize for new examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Classifiers\n",
    "\n",
    "For this project, two classifiers were chosen to be trained with the same dataset. The first classifier is the _Naive Bayes_ and the second one is _Support Vector Machines_.\n",
    "\n",
    "## Naive Bayes\n",
    "\n",
    "_Naive Bayes_ classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. Even if the features depend on each other or upon the existence of the other features, all of these properties independently contribute to the probability that this fruit is an apple and that is why it is known as ‘Naive’.\n",
    "\n",
    "_Naive Bayes_ model is easy to build and particularly useful for very large data sets. Along with simplicity, _Naive Bayes_ is known to outperform even highly sophisticated classification methods.\n",
    "\n",
    "Below is the implementation of the _Naive Bayes_.\n",
    "\n",
    "### Pros and Cons associated with Naive Bayes\n",
    "\n",
    "#### _Pros_\n",
    "\n",
    "- It is easy and fast to predict class of test data set. It also perform well in multi class prediction\n",
    "- When assumption of independence holds, a Naive Bayes classifier performs better compare to other models like logistic regression and you need less training data\n",
    "- It perform well in case of categorical input variables compared to numerical variable(s). For numerical variable, normal distribution is assumed (bell curve, which is a strong assumption)\n",
    "\n",
    "\n",
    "#### _Cons_\n",
    "\n",
    "- If categorical variable has a category (in test data set), which was not observed in training data set, then model will assign a zero probability and will be unable to make a prediction (often known as “Zero Frequency”). To solve this, it is possible to use the smoothing technique, one of the simplest smoothing techniques is called Laplace estimation\n",
    "- On the other side Naive Bayes is also known as a bad estimator, so the probability outputs from `predict_proba` are not to be taken too seriously\n",
    "- Another limitation of Naive Bayes is the assumption of independent predictors. In real life, it is almost impossible that we get a set of predictors which are completely independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Implement Naive Bayes\n",
    "def train_naive_bayes_classifer(X_train, X_test, y_train):\n",
    "\n",
    "    # Define the classifier\n",
    "    gnb = BernoulliNB(alpha=1.0, binarize=None)\n",
    "    \n",
    "    # Check the training time for the SVC\n",
    "    t=time.time()\n",
    "    gnb.fit(X_train, y_train)\n",
    "    t2 = time.time()\n",
    "    print(round(t2-t, 2), 'Seconds to train Naive Bayes...')\n",
    "    \n",
    "    # Check the score of the SVC\n",
    "    y_predict=gnb.predict(X_test)\n",
    "\n",
    "    return y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Support Vector Machine\n",
    "_Support Vector Machine_ (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. However,  it is mostly used in classification problems.\n",
    "\n",
    "SVM is mostly useful in non-linear separation problems, because it has a technique called the __kernel__ trick, these are functions which takes low dimensional input space and transform it to a higher dimensional space, which means that it converts not separable problem to separable problem.\n",
    "\n",
    "Below is the implementation of the _Support Vector Machine_.\n",
    "\n",
    "### Pros and Cons associated with SVM\n",
    "\n",
    "####  _Pros_\n",
    "- It works really well with clear margin of separation\n",
    "- It is effective in high dimensional spaces\n",
    "- It is effective in cases where number of dimensions is greater than the number of samples\n",
    "- It uses a subset of training points in the decision function (called support vectors), so it is also memory efficient\n",
    "\n",
    "\n",
    "####  _Cons_\n",
    "- It does not perform well, when we have large data set because the required training time is higher\n",
    "- It also does not perform very well, when the data set has more noise, which means that target classes are overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Implement Support Vector Machine\n",
    "def train_svm_classifer(X_train, X_test, y_train):\n",
    "    \n",
    "    # Define the classifier\n",
    "    svc = svm.LinearSVC(C=1.0) \n",
    "    \n",
    "    # Check the training time for the SVC\n",
    "    t=time.time()\n",
    "    svc.fit(X_train, y_train)\n",
    "    t2 = time.time()\n",
    "    print(round(t2-t, 2), 'Seconds to train SVM...')\n",
    "    \n",
    "    # Check the score of the SVC\n",
    "    y_predict=svc.predict(X_test)\n",
    "\n",
    "    return y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Metrics\n",
    "Here is defined the function to calculate the metrics with will be used to compare both algorithms used. They will be compared using the following metrics:\n",
    "\n",
    "* _True positives_ (__TP__) - Malware correctly classified\n",
    "* _True negatives_ (__TN__) - Non malware correctly classified\n",
    "* _False positives_ (__FP__) - Non malware classified as malware\n",
    "* _False negative_ (__FN__) - Malware classified as non malware\n",
    "\n",
    "Besides, we can use this data to calculate some more interesting metrics:\n",
    "\n",
    "### Precision\n",
    "Percentage of real malware detected in relation to all classified as malware.\n",
    "\\begin{equation*}\n",
    "Precision = \\frac{TP}{TP + FP}\n",
    "\\end{equation*}\n",
    "\n",
    "### Recall\n",
    "Percentage of real malware detected in relation to all malware in the set.\n",
    "\\begin{equation*}\n",
    "Recall = \\frac{TP}{TP + FN}\n",
    "\\end{equation*}\n",
    "\n",
    "### False positive rate\n",
    "Percentage of wrongly files classified as malware in relation to all benign files.\n",
    "\\begin{equation*}\n",
    "False Positive Rate = \\frac{FP}{FP + TN}\n",
    "\\end{equation*}\n",
    "\n",
    "### Accuracy\n",
    "Percentage of files correctly classified.\n",
    "\\begin{equation*}\n",
    "Accuracy = \\frac{TP+TN}{TP + FN + TN + FP}\n",
    "\\end{equation*}\n",
    "\n",
    "### F-measure\n",
    "Weighted average of precision and recall.\n",
    "\\begin{equation*}\n",
    "FMeasure = \\frac{2*Precision*Recall}{Precision+Recall}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Function to calculate the metrics\n",
    "def metrics(y_test, y_predict):\n",
    "    \n",
    "    # Number of examples\n",
    "    n = len(y_test)\n",
    "    \n",
    "    # Calculate true positives\n",
    "    true_positive = \\\n",
    "        sum([y_test[i] and y_predict[i] for i in range(n)])\n",
    "    print('\\nTrue positive', true_positive)\n",
    "    \n",
    "    # Calculate false positives\n",
    "    false_positive = \\\n",
    "        sum([not(y_test[i]) and y_predict[i] for i in range(n)])\n",
    "    print('False positive', false_positive)\n",
    "    \n",
    "    # Calculate false negatives\n",
    "    false_negative = \\\n",
    "        sum([y_test[i] and not(y_predict[i]) for i in range(n)])\n",
    "    print('False negative', false_negative)\n",
    "    \n",
    "    # Calculate true negatives\n",
    "    true_negative = \\\n",
    "        sum([not(y_test[i]) and not(y_predict[i]) for i in range(n)])\n",
    "    print('True positive', true_negative)\n",
    "    \n",
    "    # Calculate precision\n",
    "    precision = true_positive/(true_positive + false_positive)\n",
    "    print('\\nPrecision', round(precision*100,2), '%')\n",
    "    \n",
    "    # Calculate recall\n",
    "    recall = true_positive/(true_positive + false_negative)\n",
    "    print('Recall', round(recall*100,2), '%')\n",
    "    \n",
    "    # Calculate false positive rate\n",
    "    false_pos_rate = false_positive/(false_positive + true_negative)\n",
    "    print('False positive rate', round(false_pos_rate*100,2), '%')\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = (true_positive + true_negative)/ \\\n",
    "                    (false_positive + false_negative + \\\n",
    "                     true_positive + true_negative)\n",
    "    print('Accuracy', round(accuracy*100,2), '%')\n",
    "\n",
    "    # Calculate accuracy\n",
    "    f_measure = 2 *(precision * recall)/(precision + recall)\n",
    "    print('F-measure', round(f_measure*100,2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Results\n",
    "After having all the functions for the analysis explained and implemented, it is possible to train it with the training data and validate with the test data.\n",
    "\n",
    "Starting with the _Naive Bayes_ algorithm, it is possible to check that it is was obtained a accuracy of approximately 92.4%, what is good, however, the number for false positive was a bit high, above 9%.\n",
    "\n",
    "After obtaining this result, it was decided to train with a different classifier, in this case SVM, because this is a good application for this algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.91 Seconds to train Naive Bayes...\n",
      "\n",
      "True positive 1066\n",
      "False positive 102\n",
      "False negative 67\n",
      "True positive 989\n",
      "\n",
      "Precision 91.27 %\n",
      "Recall 94.09 %\n",
      "False positive rate 9.35 %\n",
      "Accuracy 92.4 %\n",
      "F-measure 92.66 %\n"
     ]
    }
   ],
   "source": [
    "y_predict = train_naive_bayes_classifer(X_train, X_test, y_train)\n",
    "metrics(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It is possible to notice that for the SVM, the numbers got much better with a smaller time for training. Here the accuracy is almost 97% with a false positive rate a little bit above 4%.\n",
    "\n",
    "In this case, the SVM algorithm presented a better performance for the classification of malware and non-malware examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.76 Seconds to train SVM...\n",
      "\n",
      "True positive 1107\n",
      "False positive 44\n",
      "False negative 26\n",
      "True positive 1047\n",
      "\n",
      "Precision 96.18 %\n",
      "Recall 97.71 %\n",
      "False positive rate 4.03 %\n",
      "Accuracy 96.85 %\n",
      "F-measure 96.94 %\n"
     ]
    }
   ],
   "source": [
    "y_predict = train_svm_classifer(X_train, X_test, y_train)\n",
    "metrics(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Further improvements\n",
    "\n",
    "For further improvements of this algorithm it would be interesting to better tune the parameters of the classifiers, try to discard features that does not add relevant information, therefore simplify the problem, and also, instead of just check if a word is in the file or not, count its number of occurrences in each file.\n",
    "\n",
    "Besides, an improvement can be done on the features extraction, the algorithm presented here is using just the most important word of each line of the document, for example `touchscreen` and `main`, and discarding words less relevant (`android`, `hardware`, `intent` and `action`) as the lines shown below. It would be interesting to try using all the words and, after that, study if this words can add any useful information to the classifier or are irrelevant.\n",
    "\n",
    "```\n",
    "feature::android.hardware.touchscreen\n",
    "intent::android.intent.action.MAIN\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
