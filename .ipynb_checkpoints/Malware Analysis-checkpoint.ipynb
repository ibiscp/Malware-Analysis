{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Malware Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Import necessary libraries for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os, os.path\n",
    "from collections import defaultdict, OrderedDict\n",
    "from urllib.parse import urlsplit\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# Set seed for random numbers\n",
    "random.seed(1)\n",
    "\n",
    "# Define folder where the log files are located\n",
    "folder = 'drebin/feature_vectors/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Show basic information about the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset:  129013\n",
      "Number of non malwares:\t 123453\n",
      "Number of malwares:\t 5560\n"
     ]
    }
   ],
   "source": [
    "non_malware = list()\n",
    "malware = list()\n",
    "\n",
    "dataset = os.listdir(folder) # List of all the files\n",
    "\n",
    "# Create malware dictionary with the file name and the type of each one\n",
    "with open('drebin/sha256_family.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)\n",
    "    malware_dictionary = {row[0]:row[1] for row in reader}\n",
    "\n",
    "# Separate the file names between malware and non malware\n",
    "for i in dataset:\n",
    "    if i in malware_dictionary:\n",
    "        malware.append(i)\n",
    "    else:\n",
    "        non_malware.append(i)\n",
    "    \n",
    "print('Size of dataset: ', len(malware) + len(non_malware))\n",
    "print('Number of non malwares:\\t', len(non_malware))\n",
    "print('Number of malwares:\\t', len(malware))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Malware categories\n",
    "Just to have an idea of the dataset, below is presented a list of how many samples there are in each malware class. This data will not be used for this project, but further development could be done in order to classify the class each malware belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of entries in each class of malware (values above 20):\n",
      "\t 925 FakeInstaller\n",
      "\t 667 DroidKungFu\n",
      "\t 625 Plankton\n",
      "\t 613 Opfake\n",
      "\t 339 GinMaster\n",
      "\t 330 BaseBridge\n",
      "\t 152 Iconosys\n",
      "\t 147 Kmin\n",
      "\t 132 FakeDoc\n",
      "\t  92 Geinimi\n",
      "\t  91 Adrd\n",
      "\t  81 DroidDream\n",
      "\t  70 ExploitLinuxLotoor\n",
      "\t  69 Glodream\n",
      "\t  69 MobileTx\n",
      "\t  61 FakeRun\n",
      "\t  59 SendPay\n",
      "\t  58 Gappusin\n",
      "\t  43 Imlog\n",
      "\t  41 SMSreg\n",
      "\t  37 Yzhc\n",
      "\t  29 Jifake\n",
      "\t  28 Hamob\n",
      "\t  27 Boxer\n",
      "\t 775 Others\n"
     ]
    }
   ],
   "source": [
    "print('\\nNumber of entries in each class of malware (values above 20):')\n",
    "\n",
    "v = defaultdict(list)\n",
    "for key, value in sorted(malware_dictionary.items()):\n",
    "    v[value].append(key)\n",
    "ordered_v = OrderedDict(sorted(v.items(), key=lambda x: len(x[1]), reverse=True))\n",
    "count_malware = 0\n",
    "for k in ordered_v:\n",
    "    if len(v[k])>20: # Print only classes with more than 20 examples\n",
    "        count_malware += len(v[k])\n",
    "        print('\\t', '{:>3}'.format(len(v[k])), k)\n",
    "        \n",
    "print('\\t', '{:>3}'.format(len(malware) - count_malware), 'Others')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the training and test set\n",
    "It is possible to notice that the number of non malware data in the dataset is much larger than the number of malware examples. In order to have a balanced dataset, random number of non malware examples will be selected from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate random numbers\n",
    "index = random.sample(range(0, len(non_malware)-1), len(malware))\n",
    "\n",
    "# New list with malware and non malware examples divided \n",
    "non_malware = [non_malware[i] for i in index]\n",
    "\n",
    "# Merged list containing malware and non malware\n",
    "data = malware + non_malware\n",
    "\n",
    "# Vector with class of each example\n",
    "y = [1]*len(malware) + [0]*len(non_malware)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## List of categories\n",
    "This is an example of one file containing the features extracted from the application. These features are extracted from the `manifest.xml` file and from the disassembled code.\n",
    "\n",
    "```\n",
    "00a25c24961da2d9ac3824df93deeb99ef5b8e045d3dcdeb8a0afa30184c69bd\n",
    "\n",
    "feature::android.hardware.touchscreen\n",
    "api_call::android/content/Context;->startService\n",
    "activity::.BatteryClockActivity\n",
    "activity::Choice Application !!\n",
    "intent::android.intent.action.MAIN\n",
    "activity::.LaunchDialog\n",
    "intent::android.intent.action.USER_PRESENT\n",
    "intent::android.intent.category.LAUNCHER\n",
    "real_permission::android.permission.VIBRATE\n",
    "api_call::android/app/NotificationManager;->notify\n",
    "provider::android.appwidget.provider\n",
    "call::getSystemService\n",
    "service_receiver::.BatteryClockService\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The features can be divided in 10 different sets:\n",
      "\t api_call\n",
      "\t feature\n",
      "\t url\n",
      "\t service_receiver\n",
      "\t permission\n",
      "\t call\n",
      "\t intent\n",
      "\t real_permission\n",
      "\t activity\n",
      "\t provider\n"
     ]
    }
   ],
   "source": [
    "category_list = list()\n",
    "\n",
    "for file in dataset[0:20]:\n",
    "    with open(folder + file) as f:\n",
    "        content = f.readlines()\n",
    "        for line in content:\n",
    "            category, string = line.split('::')\n",
    "            if category not in category_list:\n",
    "                category_list.append(category)\n",
    "print('The features can be divided in 10 different sets:')\n",
    "print('\\t','\\n\\t '.join(category_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the list of features to be considered is declared, in order to change the features to be considered it is necessary to set the flags to `True` or `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = {\n",
    "    'api_call': True,\n",
    "    'feature': True,\n",
    "    'url': False,\n",
    "    'service_receiver': True,\n",
    "    'permission': True,\n",
    "    'call': True,\n",
    "    'intent': True,\n",
    "    'real_permission': True,\n",
    "    'activity': True,\n",
    "    'provider': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Extract useful data\n",
    "The functions listed below are used to extract the data necessary to classify the malware. It gets each line of the files based on the category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Extract url\n",
    "def extract_url(string):\n",
    "    try:\n",
    "        #string = ''.join(e for e in string if e.isalnum())\n",
    "        base_url = \"{0.scheme}://{0.netloc}/\".format(urlsplit(string))\n",
    "        if len(base_url) > 10:\n",
    "            return [base_url]\n",
    "    except:\n",
    "        #print('Error html: ', string)\n",
    "        return None\n",
    "    \n",
    "# Extract api_call\n",
    "def extract_api_call(string):\n",
    "    try:\n",
    "        string = string.replace(';->', '/')\n",
    "        api_call = string.split('/')\n",
    "        return api_call\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "# Extract feature\n",
    "def extract_feature(string):\n",
    "    try:\n",
    "        feature = string.split('.')[-1]\n",
    "        return [feature]\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "# Extract permission and real_permission\n",
    "def extract_permission(string):\n",
    "    try:\n",
    "        permission = string.split('.')[-1].lower()\n",
    "        return [permission]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Extract call\n",
    "def extract_call(string):\n",
    "    try:\n",
    "        call = string.lower()\n",
    "        '''par = call.find( '(' )\n",
    "        if par != -1:\n",
    "            call = call[:par]'''\n",
    "        return [call]\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "# Extract activity\n",
    "def extract_activity(string):\n",
    "    try:\n",
    "        activity = string.split('.')[-1].lower()\n",
    "        return [activity]\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "# Extract intent\n",
    "def extract_intent(string):\n",
    "    try:\n",
    "        intent = string.split('.')[-1].lower()\n",
    "        return [intent]\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "# Extract service_receiver\n",
    "def extract_service_receiver(string):\n",
    "    try:\n",
    "        service_receiver = string.split('.')[-1].lower()\n",
    "        return [service_receiver]\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "# Extract provider\n",
    "def extract_provider(string):\n",
    "    try:\n",
    "        provider = string.split('.')[-1].lower()\n",
    "        return [provider]\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map words to index\n",
    "It is important to create a dictionary with all the words containing in the files and convert each file to a vector of the index of each word in the dictionary, as show in the example below:\n",
    "```\n",
    "000068216bdb459df847bfdd67dd11069c3c50166db1ea8772cdc9250d948bcf\n",
    "[3, 55, 56, 11, 13, 57, 22, 58, 59, 52, 60, 61, 4, 5, 6, 62, 63, 64, 49, 50, 51, 40, 41, 65, 47, 53]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_file(file, dictionary_creation = False):\n",
    "\n",
    "    # List of words of each file\n",
    "    words = list()\n",
    "    \n",
    "    # Read line by line of the file\n",
    "    with open(folder + file) as f:\n",
    "        content = f.readlines()\n",
    "        \n",
    "        # Divide each line of document in \n",
    "        for line in content:\n",
    "            try:\n",
    "                split = line.split('::')\n",
    "                category = split[0]\n",
    "                string = split[1]\n",
    "            except:\n",
    "                break\n",
    "                \n",
    "            # Only process the categories selected by the user\n",
    "            if (features[category]):\n",
    "                if (category == 'url'):\n",
    "                    word_list = extract_url(string)\n",
    "                elif (category == 'api_call'):\n",
    "                    word_list = extract_api_call(string)\n",
    "                elif (category == 'feature'):\n",
    "                    word_list = extract_feature(string)\n",
    "                elif (category == 'permission' or category == 'real_permission'):\n",
    "                    word_list = extract_permission(string)\n",
    "                elif (category == 'call'):\n",
    "                    word_list = extract_call(string)\n",
    "                elif (category == 'activity'):\n",
    "                    word_list = extract_activity(string)\n",
    "                elif (category == 'intent'):\n",
    "                    word_list = extract_intent(string)\n",
    "                elif (category == 'service_receiver'):\n",
    "                    word_list = extract_service_receiver(string)\n",
    "                elif (category == 'provider'):\n",
    "                    word_list = extract_provider(string)\n",
    "\n",
    "                if word_list != None:\n",
    "                    for word in word_list:\n",
    "                        word = word.replace('\\n', '')\n",
    "\n",
    "                        if dictionary_creation:\n",
    "                            index = len(dictionary)-1\n",
    "                            dictionary[word] = index\n",
    "                        else:\n",
    "                            index = dictionary[word]\n",
    "                            if index not in words:\n",
    "                                words.append(index)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dictionary\n",
    "In order to classify the words, first the dictionary needs to be created using the words of the files that it will classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary file not found!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating dictionary: 100%|███████████████████████████████████████████████████████| 11120/11120 [03:52<00:00, 47.92it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Print' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-09b8297b6cba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mprocess_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mPrint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Dictionary saved to file!\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m# Save dictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Print' is not defined"
     ]
    }
   ],
   "source": [
    "# Save dictionary to file\n",
    "def save_dic(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Load dictionary from file\n",
    "def load_dic(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# Check if dictionary exists\n",
    "if os.path.isfile('dictionary.pkl'):\n",
    "    print('Dictionary file found!')\n",
    "    load_dic('dictionary')\n",
    "\n",
    "else:\n",
    "    print('Dictionary file not found!')\n",
    "    \n",
    "    # Define the dictionary\n",
    "    dictionary = {}\n",
    "    \n",
    "    # Colect words for malware\n",
    "    pbar = tqdm(range(len(data)))\n",
    "    pbar.set_description('Creating dictionary')\n",
    "    for i in pbar:\n",
    "        process_file(data[i], True)\n",
    "        \n",
    "    # Save dictionary\n",
    "    save_dic(dictionary, 'dictionary')\n",
    "    print('Dictionary saved to file!\\n')\n",
    "    \n",
    "dictionary_size = len(dictionary)\n",
    "print('Size of dictionary: ', dictionary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                        | 0/11120 [00:00<?, ?it/s]\n",
      "Extracting features:   0%|                                                                   | 0/11120 [00:00<?, ?it/s]\n",
      "Extracting features:   2%|▊                                                      | 169/11120 [00:00<00:06, 1663.91it/s]\n",
      "Extracting features:   3%|█▌                                                     | 324/11120 [00:00<00:06, 1598.80it/s]\n",
      "Extracting features:   4%|██▍                                                    | 483/11120 [00:00<00:06, 1597.97it/s]\n",
      "Extracting features:   6%|███▏                                                   | 651/11120 [00:00<00:06, 1612.06it/s]\n",
      "Extracting features:   7%|███▊                                                   | 775/11120 [00:00<00:07, 1357.86it/s]\n",
      "Extracting features:   8%|████▋                                                  | 941/11120 [00:00<00:07, 1402.71it/s]\n",
      "Extracting features:  10%|█████▍                                                | 1111/11120 [00:00<00:06, 1442.03it/s]\n",
      "Extracting features:  11%|██████▏                                               | 1265/11120 [00:00<00:06, 1451.28it/s]\n",
      "Extracting features:  13%|██████▊                                               | 1409/11120 [00:01<00:07, 1337.58it/s]\n",
      "Extracting features:  14%|███████▌                                              | 1566/11120 [00:01<00:07, 1358.78it/s]\n",
      "Extracting features:  16%|████████▍                                             | 1726/11120 [00:01<00:06, 1377.93it/s]\n",
      "Extracting features:  17%|█████████▏                                            | 1895/11120 [00:01<00:06, 1400.91it/s]\n",
      "Extracting features:  18%|█████████▉                                            | 2045/11120 [00:01<00:06, 1334.55it/s]\n",
      "Extracting features:  20%|██████████▊                                           | 2214/11120 [00:01<00:06, 1357.06it/s]\n",
      "Extracting features:  21%|███████████▌                                          | 2369/11120 [00:01<00:06, 1367.36it/s]\n",
      "Extracting features:  23%|████████████▏                                         | 2515/11120 [00:01<00:06, 1372.69it/s]\n",
      "Extracting features:  24%|████████████▉                                         | 2661/11120 [00:01<00:06, 1373.33it/s]\n",
      "Extracting features:  25%|█████████████▌                                        | 2805/11120 [00:02<00:06, 1310.29it/s]\n",
      "Extracting features:  27%|██████████████▎                                       | 2954/11120 [00:02<00:06, 1317.68it/s]\n",
      "Extracting features:  28%|███████████████                                       | 3100/11120 [00:02<00:06, 1323.87it/s]\n",
      "Extracting features:  29%|███████████████▊                                      | 3261/11120 [00:02<00:05, 1334.73it/s]\n",
      "Extracting features:  31%|████████████████▌                                     | 3404/11120 [00:02<00:05, 1333.81it/s]\n",
      "Extracting features:  32%|█████████████████▏                                    | 3543/11120 [00:02<00:05, 1276.25it/s]\n",
      "Extracting features:  33%|█████████████████▊                                    | 3659/11120 [00:02<00:05, 1264.42it/s]\n",
      "Extracting features:  34%|██████████████████▎                                   | 3770/11120 [00:02<00:05, 1257.58it/s]\n",
      "Extracting features:  35%|██████████████████▉                                   | 3887/11120 [00:03<00:05, 1254.42it/s]\n",
      "Extracting features:  36%|███████████████████▋                                  | 4042/11120 [00:03<00:05, 1263.78it/s]\n",
      "Extracting features:  37%|████████████████████▏                                 | 4167/11120 [00:03<00:05, 1230.55it/s]\n",
      "Extracting features:  38%|████████████████████▊                                 | 4275/11120 [00:03<00:05, 1225.17it/s]\n",
      "Extracting features:  39%|█████████████████████▎                                | 4382/11120 [00:03<00:05, 1214.43it/s]\n",
      "Extracting features:  40%|█████████████████████▊                                | 4484/11120 [00:03<00:05, 1205.90it/s]\n",
      "Extracting features:  41%|██████████████████████▋                                | 4583/11120 [00:04<00:06, 994.49it/s]\n",
      "Extracting features:  42%|███████████████████████                                | 4656/11120 [00:06<00:08, 747.89it/s]\n",
      "Extracting features:  42%|███████████████████████▎                               | 4708/11120 [00:07<00:09, 641.37it/s]\n",
      "Exception in thread Thread-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\ibisc\\miniconda3\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\users\\ibisc\\miniconda3\\lib\\site-packages\\tqdm\\_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"c:\\users\\ibisc\\miniconda3\\lib\\_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      "Extracting features: 100%|███████████████████████████████████████████████████████| 11120/11120 [01:58<00:00, 93.56it/s]\n"
     ]
    }
   ],
   "source": [
    "def features_extraction(file):\n",
    "    indices = process_file(file)\n",
    "    \n",
    "    feat = [0] * dictionary_size\n",
    "    \n",
    "    for i in indices:\n",
    "        feat[i-1] = 1\n",
    "\n",
    "    return feat\n",
    "\n",
    "X = list()\n",
    "\n",
    "pbar = tqdm(range(len(data)))\n",
    "pbar.set_description('Extracting features')\n",
    "for i in pbar:\n",
    "    feat = features_extraction(data[i])\n",
    "    X.append(feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-7a95143dc160>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import sklearn\n",
    "from sklearn import cross_validation, grid_search\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "def train_svm_classifer(features, labels, model_output_path):\n",
    "    \"\"\"\n",
    "    train_svm_classifer will train a SVM, saved the trained and SVM model and\n",
    "    report the classification performance\n",
    "\n",
    "    features: array of input features\n",
    "    labels: array of labels associated with the input features\n",
    "    model_output_path: path for storing the trained svm model\n",
    "    \"\"\"\n",
    "    # save 20% of data for performance evaluation\n",
    "    X_train, X_test, y_train, y_test = cross_validation.train_test_split(features, labels, test_size=0.2)\n",
    "\n",
    "    param = [\n",
    "        {\n",
    "            \"kernel\": [\"linear\"],\n",
    "            \"C\": [1, 10, 100, 1000]\n",
    "        },\n",
    "        {\n",
    "            \"kernel\": [\"rbf\"],\n",
    "            \"C\": [1, 10, 100, 1000],\n",
    "            \"gamma\": [1e-2, 1e-3, 1e-4, 1e-5]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # request probability estimation\n",
    "    svm = SVC(probability=True)\n",
    "\n",
    "    # 10-fold cross validation, use 4 thread as each fold and each parameter set can be train in parallel\n",
    "    clf = grid_search.GridSearchCV(svm, param,\n",
    "            cv=10, n_jobs=4, verbose=3)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    if os.path.exists(model_output_path):\n",
    "        joblib.dump(clf.best_estimator_, model_output_path)\n",
    "    else:\n",
    "        print(\"Cannot save trained svm model to {0}.\".format(model_output_path))\n",
    "\n",
    "    print(\"\\nBest parameters set:\")\n",
    "    print(clf.best_params_)\n",
    "\n",
    "    y_predict=clf.predict(X_test)\n",
    "\n",
    "    labels=sorted(list(set(labels)))\n",
    "    print(\"\\nConfusion matrix:\")\n",
    "    #print(\"Labels: {0}\\n\".format(\",\".join(labels)))\n",
    "    print(confusion_matrix(y_test, y_predict, labels=labels))\n",
    "\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(classification_report(y_test, y_predict))\n",
    "    \n",
    "train_svm_classifer(X, y, 'model.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
