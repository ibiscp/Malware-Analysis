{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Malware Analysis\n",
    "\n",
    "## Introduction\n",
    "The goal of this project is to train different classifiers in order to separate malware from non malware aplications running in an Android OS, given features extracted from the `manifest.xml` file and from the disassembled code.\n",
    "\n",
    "The dataset that is being used here is the [The Drebin Dataset](https://www.sec.cs.tu-bs.de/~danarp/drebin/download.html) from the Braunschweig University of Technology.\n",
    "\n",
    "The dataset contains 5,560 files from 179 different malware families. The samples were collected in the period of August 2010 to October 2012. More details on the dataset can be found in the [paper](https://www.tu-braunschweig.de/Medien-DB/sec/pubs/2014-ndss.pdf) describing Drebin and the corresponding evaluation.\n",
    "\n",
    "## Development\n",
    "Below the development of the project is detailed, commenting each of the functions and steps given to achieve the desired goals for the project.\n",
    "\n",
    "### Import necessary libraries for the project\n",
    "In the following block is listed all the necessary libraries for the project, the seed for the random numbers and the path for the folder containing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Difine libraries\n",
    "import csv\n",
    "import os, os.path\n",
    "from collections import defaultdict, OrderedDict\n",
    "from urllib.parse import urlsplit\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "# Set seed for random numbers\n",
    "random.seed(1)\n",
    "\n",
    "# Define folder where the log files are located\n",
    "folder = 'drebin/feature_vectors/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Understending the dataset\n",
    "As described above, the dataset is composed of examples of malware and non-malware data, each file contains features of requested hardware components, requested permissions, app components, netword addresses and so on. An example of one file is presented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_file = 'd4e03d6e85b2f45a754240136d7544351b733fb12d9bb7511227da31bc709399'\n",
    "    \n",
    "print('File: ', sample_file, '\\n')\n",
    "\n",
    "# Print its content\n",
    "with open(folder + sample_file) as f:\n",
    "    content = f.read().splitlines()\n",
    "    for line in content:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to start to have a better understanding about the dataset, it is interesting to find out the total number of malware and non-malware files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define list for the malware and non malware files\n",
    "non_malware = list()\n",
    "malware = list()\n",
    "\n",
    "# List of all the files\n",
    "dataset = os.listdir(folder)\n",
    "\n",
    "# Create malware dictionary with the file name and the type of each one\n",
    "with open('drebin/sha256_family.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)\n",
    "    malware_dictionary = {row[0]:row[1] for row in reader}\n",
    "\n",
    "# Separate the file names between malware and non malware\n",
    "for i in dataset:\n",
    "    if i in malware_dictionary:\n",
    "        malware.append(i)\n",
    "    else:\n",
    "        non_malware.append(i)\n",
    "    \n",
    "print('Size of dataset: ', len(malware) + len(non_malware))\n",
    "print('Number of non malwares:\\t', len(non_malware))\n",
    "print('Number of malwares:\\t', len(malware))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to notice that the dataset is not balanced, in this dataset, the number of non malware files is much larger than the number of malware, and this can influence on the classification. So, before to start it is good to pick ramdomly the same number of files from the non malware class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Balancing the dataset\n",
    "Randomly select the same number of malware examples from the non malware in order to have a balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Generate random numbers\n",
    "index = random.sample(range(0, len(non_malware)-1), len(malware))\n",
    "\n",
    "# New list with malware and non malware examples divided \n",
    "non_malware = [non_malware[i] for i in index]\n",
    "\n",
    "# Merged list containing malware and non malware\n",
    "data = malware + non_malware\n",
    "\n",
    "# Vector with class of each example\n",
    "y = [1]*len(malware) + [0]*len(non_malware)\n",
    "\n",
    "# Print number of examples in each class\n",
    "print('Number of non malware: ', len(non_malware))\n",
    "print('Number of malware: ', len(malware))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now both classes have the same number of examples, both with 5560 files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Malware categories\n",
    "Just to have an idea of the dataset, below is presented a list of how many samples there are in each malware class. This data will not be used for this project, but further development can be done in order to classify the class each malware belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('\\nNumber of entries in each class of malware (values above 20):')\n",
    "\n",
    "v = defaultdict(list)\n",
    "for key, value in sorted(malware_dictionary.items()):\n",
    "    v[value].append(key)\n",
    "ordered_v = OrderedDict(sorted(v.items(), key=lambda x: len(x[1]), reverse=True))\n",
    "count_malware = 0\n",
    "for k in ordered_v:\n",
    "    if len(v[k])>20: # Print only classes with more than 20 examples\n",
    "        count_malware += len(v[k])\n",
    "        print('\\t', '{:>3}'.format(len(v[k])), k)\n",
    "        \n",
    "print('\\t', '{:>3}'.format(len(malware) - count_malware), 'Others')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Features categories\n",
    "\n",
    "As explained above, each file contains all the features extracted from the application. These features are extracted from the `manifest.xml` file and from the disassembled code.\n",
    "\n",
    "Below is presented the total number of categories found in the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Collect all the features found in the first 20 files of the dataset\n",
    "category_list = list()\n",
    "for file in dataset[0:20]:\n",
    "    with open(folder + file) as f:\n",
    "        content = f.readlines()\n",
    "        for line in content:\n",
    "            category, string = line.split('::')\n",
    "            if category not in category_list:\n",
    "                category_list.append(category)\n",
    "                \n",
    "print('The features can be divided in', len(category_list), 'different sets:')\n",
    "print('\\t','\\n\\t '.join(category_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In order to make the problem easier or more difficult to solve, it is possible to chose which features will be considered in order to classify between the two classes.\n",
    "\n",
    "In the list of features below it is possible to set `True` or `False` for each one of them, `True` to be considered by the classifier and `False` to be discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "features = {\n",
    "    'api_call': True,\n",
    "    'feature': True,\n",
    "    'url': True,\n",
    "    'service_receiver': True,\n",
    "    'permission': True,\n",
    "    'call': True,\n",
    "    'intent': True,\n",
    "    'real_permission': True,\n",
    "    'activity': True,\n",
    "    'provider': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Functions to extract features\n",
    "The functions listed below are used to extract the data necessary from the features. It receives each line of the files based on the category and returns a sequence of words that can be used to create the dictionary and/or construct the vectore of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Extract url\n",
    "def extract_url(string):\n",
    "    try:\n",
    "        base_url = \"{0.scheme}://{0.netloc}/\".format(urlsplit(string))\n",
    "        if len(base_url) > 10:\n",
    "            return [base_url]\n",
    "    except:\n",
    "        #print('Error html: ', string)\n",
    "        return None\n",
    "    \n",
    "# Extract api_call\n",
    "def extract_api_call(string):\n",
    "    try:\n",
    "        string = string.replace(';->', '/')\n",
    "        api_call = string.split('/')\n",
    "        return api_call\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "# Extract feature\n",
    "def extract_feature(string):\n",
    "    try:\n",
    "        feature = string.split('.')[-1]\n",
    "        return [feature]\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "# Extract permission and real_permission\n",
    "def extract_permission(string):\n",
    "    try:\n",
    "        permission = string.split('.')[-1].lower()\n",
    "        return [permission]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Extract call\n",
    "def extract_call(string):\n",
    "    try:\n",
    "        call = string.lower()\n",
    "        return [call]\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "# Extract activity\n",
    "def extract_activity(string):\n",
    "    try:\n",
    "        activity = string.split('.')[-1].lower()\n",
    "        return [activity]\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "# Extract intent\n",
    "def extract_intent(string):\n",
    "    try:\n",
    "        intent = string.split('.')[-1].lower()\n",
    "        return [intent]\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "# Extract service_receiver\n",
    "def extract_service_receiver(string):\n",
    "    try:\n",
    "        service_receiver = string.split('.')[-1].lower()\n",
    "        return [service_receiver]\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "# Extract provider\n",
    "def extract_provider(string):\n",
    "    try:\n",
    "        provider = string.split('.')[-1].lower()\n",
    "        return [provider]\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Map words to index\n",
    "It is important to create a dictionary with all the words containing in the files and convert each file to a vector of the index of each word in the dictionary, as show in the example below:\n",
    "```\n",
    "000068216bdb459df847bfdd67dd11069c3c50166db1ea8772cdc9250d948bcf\n",
    "[3, 55, 56, 11, 13, 57, 22, 58, 59, 52, 60, 61, 4, 5, 6, 62, 63, 64, 49, 50, 51, 40, 41, 65, 47, 53]\n",
    "```\n",
    "The following function receives a file, reads its line, process using the functions to extract features and creates or the dictionary or the vector shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_file(file, dictionary_creation = False):\n",
    "\n",
    "    # List of words of each file\n",
    "    words = list()\n",
    "    \n",
    "    # Read line by line of the file\n",
    "    with open(folder + file) as f:\n",
    "        content = f.readlines()\n",
    "        \n",
    "        # Divide each line of document in \n",
    "        for line in content:\n",
    "            try:\n",
    "                split = line.split('::')\n",
    "                category = split[0]\n",
    "                string = split[1]\n",
    "            except:\n",
    "                break\n",
    "                \n",
    "            # Only process the categories selected by the user\n",
    "            if (features[category]):\n",
    "                if (category == 'url'):\n",
    "                    word_list = extract_url(string)\n",
    "                elif (category == 'api_call'):\n",
    "                    word_list = extract_api_call(string)\n",
    "                elif (category == 'feature'):\n",
    "                    word_list = extract_feature(string)\n",
    "                elif (category == 'permission' or category == 'real_permission'):\n",
    "                    word_list = extract_permission(string)\n",
    "                elif (category == 'call'):\n",
    "                    word_list = extract_call(string)\n",
    "                elif (category == 'activity'):\n",
    "                    word_list = extract_activity(string)\n",
    "                elif (category == 'intent'):\n",
    "                    word_list = extract_intent(string)\n",
    "                elif (category == 'service_receiver'):\n",
    "                    word_list = extract_service_receiver(string)\n",
    "                elif (category == 'provider'):\n",
    "                    word_list = extract_provider(string)\n",
    "\n",
    "                # If able to extract feature from line\n",
    "                if word_list != None:\n",
    "                    for word in word_list:\n",
    "                        word = word.replace('\\n', '')\n",
    "\n",
    "                        # If flagged to create the dictionary\n",
    "                        if dictionary_creation:\n",
    "                            index = len(dictionary)-1\n",
    "                            dictionary[word] = index\n",
    "                        else:\n",
    "                            index = dictionary[word]\n",
    "                            if index not in words:\n",
    "                                words.append(index)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Dictionary creation\n",
    "In order to classify the files, first a dictionary needs to be created using the words of the files that it will classify. For the dictionary all the words containing in each file is being used for each of the categories showed above.\n",
    "As this creating takes time, because it is necessary to read all the words containing in each file, the dictionary is being save to a file called `dictionary.pkl`. It will only be create if it is not found on the directory folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Save dictionary to file\n",
    "def save_dic(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Load dictionary from file\n",
    "def load_dic(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# Check if dictionary exists\n",
    "if os.path.isfile('dictionary.pkl'):\n",
    "    print('Dictionary file found!')\n",
    "    dictionary = load_dic('dictionary')\n",
    "\n",
    "else:\n",
    "    print('Dictionary file not found!')\n",
    "    \n",
    "    # Define the dictionary\n",
    "    dictionary = {}\n",
    "    \n",
    "    # Colect words for malware\n",
    "    pbar = tqdm(range(len(data)))\n",
    "    pbar.set_description('Creating dictionary')\n",
    "    for i in pbar:\n",
    "        process_file(data[i], True)\n",
    "        \n",
    "    # Save dictionary\n",
    "    save_dic(dictionary, 'dictionary')\n",
    "    print('Dictionary saved to file!\\n')\n",
    "\n",
    "# Print number of words in the dictionary\n",
    "dictionary_size = len(dictionary)\n",
    "print('Dictionary size: ', dictionary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Features extraction\n",
    "This is the function used to extract features of each file, it uses the same function to build the dictionary, but here a vector of zeros is created with the lenght of the dictionary, and its value is set to one only in the position of the words that contains in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Construct the vector of features\n",
    "def features_extraction(file):\n",
    "    indices = process_file(file)\n",
    "    \n",
    "    feat = [0] * dictionary_size\n",
    "    \n",
    "    for i in indices:\n",
    "        feat[i-1] = 1\n",
    "\n",
    "    return feat\n",
    "\n",
    "# List of features\n",
    "X = list()\n",
    "\n",
    "# Extract features and append to the list of features\n",
    "pbar = tqdm(range(len(data)))\n",
    "pbar.set_description('Extracting features')\n",
    "for i in pbar:\n",
    "    feat = features_extraction(data[i])\n",
    "    X.append(feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_svm_classifer(features, labels):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2)\n",
    "    \n",
    "    # Use a linear SVC \n",
    "    svc = svm.LinearSVC()\n",
    "    \n",
    "    # Check the training time for the SVC\n",
    "    t=time.time()\n",
    "    svc.fit(X_train, y_train)\n",
    "    t2 = time.time()\n",
    "    print(round(t2-t, 2), 'Seconds to train SVM...')\n",
    "    \n",
    "    # Check the score of the SVC\n",
    "    y_predict=svc.predict(X_test)\n",
    "    \n",
    "    print('\\nTest Accuracy of SVM')\n",
    "    print(classification_report(y_test, y_predict))\n",
    "    \n",
    "    print('Test Accuracy of SVM = ', round(svc.score(X_test, y_test), 4))\n",
    "\n",
    "train_svm_classifer(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB#GaussianNB\n",
    "\n",
    "def train_naive_bayes_classifer(features, labels):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2)\n",
    "    \n",
    "    gnb = MultinomialNB()\n",
    "    \n",
    "    # Check the training time for the SVC\n",
    "    t=time.time()\n",
    "    gnb.fit(X_train, y_train)\n",
    "    t2 = time.time()\n",
    "    print(round(t2-t, 2), 'Seconds to train Naive Bayes...')\n",
    "    \n",
    "    # Check the score of the SVC\n",
    "    y_predict=gnb.predict(X_test)\n",
    "    \n",
    "    print('\\nTest Accuracy of Naive Bayes')\n",
    "    print(classification_report(y_test, y_predict))\n",
    "    \n",
    "    print('Test Accuracy of Naive Bayes = ', round(gnb.score(X_test, y_test), 4))\n",
    "\n",
    "train_naive_bayes_classifer(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
